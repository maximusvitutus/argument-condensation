{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "# Get the absolute path to the parent directory (assumes this file is in 'condensation')\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, parent_dir)\n",
    "# autoreload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from typing import List, Optional, Dict, Any\n",
    "import datetime\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "from chatbot_api.providers.openai import OpenAIProvider\n",
    "from chatbot_api.base import Role, Message\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Argument:\n",
    "    def __init__(self, main_point: str, subpoints: List[str], score_distribution: List[int], source_indices: List[int]):\n",
    "        self.main_point = main_point                 # The main argument which is a broad, high-level point\n",
    "        self.subpoints = subpoints                   # Supporting subpoints\n",
    "        self.score_distribution = score_distribution # Likert score distribution\n",
    "        self.source_indices = source_indices         # Indices of the source comments that support the argument (at least in some way)\n",
    "\n",
    "@dataclass\n",
    "class BatchMetadata:\n",
    "    \"\"\"Metadata for each batch processing iteration\"\"\"\n",
    "    batch_number: int\n",
    "    timestamp: datetime\n",
    "    processed_indices: List[int]\n",
    "    processed_comments: List[str]\n",
    "    arguments_after_batch: List[Argument]\n",
    "    batch_likert_scores: Optional[List[int]] = None\n",
    "    metrics: Optional[Dict[str, Any]] = None  # For storing additional metrics like processing time, token usage etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgumentProcessor:\n",
    "    def __init__(self, llm_provider: OpenAIProvider, batch_size: int):\n",
    "        self.llm_provider = llm_provider\n",
    "        self.batch_size = batch_size\n",
    "        self.current_arguments: List[Argument] = []\n",
    "        self.processing_history: List[BatchMetadata] = []\n",
    "        \n",
    "        self.INITIAL_BATCH_TEMPLATE = \"\"\"\n",
    "            ### Instructions:\n",
    "            1. Review the following numbered comments on the topic: \"{topic}\"\n",
    "            2. Identify and categorize arguments into main topics.\n",
    "                - Main arguments are broad, high-level points.\n",
    "                - Subpoints provide supporting details or reasoning for the main argument.\n",
    "            3. For each argument and subpoint, include the indices of the source comments in square brackets.\n",
    "            4. Provide the output in Finnish.\n",
    "\n",
    "            ### Output Format: \n",
    "            <ARGUMENTS>\n",
    "            MAIN: [Main Argument] [source_indices: 1,2,3]\n",
    "            SUB: [Supporting subpoint] [source_indices: 1,2]\n",
    "            SUB: [Another supporting subpoint] [source_indices: 3]\n",
    "            </ARGUMENTS>\n",
    "\n",
    "            ### Comments to analyze:\n",
    "            {comments_text}\n",
    "            \"\"\"\n",
    "            \n",
    "        self.SYNTHESIS_TEMPLATE = \"\"\"\n",
    "            ### Instructions:\n",
    "            1. Review the existing arguments and new comments on the topic: \"{topic}\"\n",
    "            2. Perform a comprehensive synthesis:\n",
    "                a. First, consider if the new comments suggest any major themes or perspectives that aren't captured in the existing arguments\n",
    "                b. Evaluate if existing main arguments should be:\n",
    "                - Combined if they represent closely related ideas\n",
    "                - Split if they contain distinct themes that deserve separate focus\n",
    "                - Reworded to better capture the full scope of ideas, including new comments\n",
    "                - Removed if they're no longer representative of the broader discussion\n",
    "                c. Only after restructuring the main arguments, organize supporting points under them\n",
    "            3. For each argument and subpoint, include ALL relevant source indices (both from existing and new comments)\n",
    "            4. Provide the output in Finnish.\n",
    "\n",
    "            ### Important:\n",
    "            - Don't feel constrained by the existing argument structure\n",
    "            - New comments might reveal better ways to organize and present the overall discussion\n",
    "            - Main arguments should reflect the most important themes across ALL comments\n",
    "\n",
    "            ### Existing Arguments:\n",
    "            {existing_arguments}\n",
    "\n",
    "            ### New Comments to Analyze:\n",
    "            {new_comments}\n",
    "\n",
    "            ### Output Format:\n",
    "            <ARGUMENTS>\n",
    "            MAIN: [Main Argument] [source_indices: 1,2,3]\n",
    "            SUB: [Supporting subpoint] [source_indices: 1,2]\n",
    "            SUB: [Another supporting subpoint] [source_indices: 3]\n",
    "            </ARGUMENTS>\n",
    "            \"\"\"\n",
    "\n",
    "    async def process_all_comments(self, \n",
    "                                comments: List[str], \n",
    "                                comment_indices: List[int], \n",
    "                                topic: str, \n",
    "                                likert_answers: Optional[List[int]] = None) -> List[Argument]:\n",
    "        \"\"\"Process all comments in batches, maintaining argument synthesis across batches.\"\"\"\n",
    "        self.processing_history = []  # Reset history for new processing run\n",
    "        \n",
    "        # Process comments in batches of batch_size\n",
    "        for batch_num, i in enumerate(range(0, len(comments), self.batch_size)):\n",
    "            batch_comments = comments[i:i + self.batch_size]\n",
    "            batch_indices = comment_indices[i:i + self.batch_size]\n",
    "            batch_likert = likert_answers[i:i + self.batch_size] if likert_answers else None\n",
    "            \n",
    "            start_time = datetime.datetime.now()\n",
    "            \n",
    "            if not self.current_arguments:\n",
    "                # First batch: Use initial template\n",
    "                new_arguments = await self._process_initial_batch(\n",
    "                    batch_comments, batch_indices, topic, batch_likert\n",
    "                )\n",
    "            else:\n",
    "                # Subsequent batches: Synthesize with existing arguments\n",
    "                new_arguments = await self._synthesize_batch(\n",
    "                    batch_comments, batch_indices, topic, batch_likert\n",
    "                )\n",
    "            \n",
    "            # Update current arguments\n",
    "            self.current_arguments = new_arguments\n",
    "            \n",
    "            # Record batch metadata\n",
    "            processing_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "            batch_metadata = BatchMetadata(\n",
    "                batch_number=batch_num,\n",
    "                timestamp=datetime.datetime.now(),\n",
    "                processed_indices=batch_indices,\n",
    "                processed_comments=batch_comments,\n",
    "                arguments_after_batch=new_arguments.copy(),\n",
    "                batch_likert_scores=batch_likert,\n",
    "                metrics={\n",
    "                    'processing_time_seconds': processing_time,\n",
    "                    'num_comments': len(batch_comments),\n",
    "                    'num_arguments': len(new_arguments),\n",
    "                    'total_subpoints': sum(len(arg.subpoints) for arg in new_arguments),\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            self.processing_history.append(batch_metadata)\n",
    "            \n",
    "        return self.current_arguments\n",
    "\n",
    "    def get_argument_evolution(self, argument_index: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Track how a specific argument evolved across batches.\n",
    "        Returns a list of the argument's main point at each iteration.\n",
    "        \"\"\"\n",
    "        evolution = []\n",
    "        for batch in self.processing_history:\n",
    "            if argument_index < len(batch.arguments_after_batch):\n",
    "                evolution.append(batch.arguments_after_batch[argument_index].main_point)\n",
    "        return evolution\n",
    "\n",
    "    def get_batch_statistics(self) -> Dict[str, List[Any]]:\n",
    "        \"\"\"\n",
    "        Get statistics about each batch processing iteration.\n",
    "        \"\"\"\n",
    "        stats = {\n",
    "            'batch_numbers': [],\n",
    "            'timestamps': [],\n",
    "            'num_comments': [],\n",
    "            'num_arguments': [],\n",
    "            'processing_times': [],\n",
    "            'total_subpoints': []\n",
    "        }\n",
    "        \n",
    "        for batch in self.processing_history:\n",
    "            stats['batch_numbers'].append(batch.batch_number)\n",
    "            stats['timestamps'].append(batch.timestamp)\n",
    "            stats['num_comments'].append(batch.metrics['num_comments'])\n",
    "            stats['num_arguments'].append(batch.metrics['num_arguments'])\n",
    "            stats['processing_times'].append(batch.metrics['processing_time_seconds'])\n",
    "            stats['total_subpoints'].append(batch.metrics['total_subpoints'])\n",
    "            \n",
    "        return stats\n",
    "\n",
    "    def export_processing_history(self, filepath: str):\n",
    "        \"\"\"\n",
    "        Export the processing history to a JSON file.\n",
    "        \"\"\"\n",
    "        history_data = []\n",
    "        for batch in self.processing_history:\n",
    "            # Convert arguments to serializable format\n",
    "            arguments_data = []\n",
    "            for arg in batch.arguments_after_batch:\n",
    "                arguments_data.append({\n",
    "                    'main_point': arg.main_point,\n",
    "                    'subpoints': arg.subpoints,\n",
    "                    'source_indices': arg.source_indices,\n",
    "                    'score_distribution': arg.score_distribution\n",
    "                })\n",
    "            \n",
    "            batch_data = {\n",
    "                'batch_number': batch.batch_number,\n",
    "                'timestamp': batch.timestamp.isoformat(),\n",
    "                'processed_indices': batch.processed_indices,\n",
    "                'processed_comments': batch.processed_comments,\n",
    "                'arguments': arguments_data,\n",
    "                'metrics': batch.metrics\n",
    "            }\n",
    "            history_data.append(batch_data)\n",
    "            \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(history_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    async def _process_initial_batch(self, \n",
    "                                   comments: List[str], \n",
    "                                   indices: List[int], \n",
    "                                   topic: str,\n",
    "                                   likert_answers: List[int] = None) -> List[Argument]:\n",
    "        \"\"\"Process the first batch of comments.\"\"\"\n",
    "        numbered_comments = [f\"[{idx}] {comment}\" for idx, comment in zip(indices, comments)]\n",
    "        comments_text = \"\\n\".join(numbered_comments)\n",
    "        \n",
    "        prompt = self.INITIAL_BATCH_TEMPLATE.format(\n",
    "            topic=topic,\n",
    "            comments_text=comments_text\n",
    "        )\n",
    "        \n",
    "        response = await self.llm_provider.generate([Message(Role.USER, prompt)])\n",
    "        return await self._parse_arguments(response.content, likert_answers)\n",
    "\n",
    "    async def _synthesize_batch(self,\n",
    "                              new_comments: List[str],\n",
    "                              new_indices: List[int],\n",
    "                              topic: str,\n",
    "                              likert_answers: Optional[List[int]] = None) -> List[Argument]:\n",
    "        \"\"\"Synthesize new comments with existing arguments.\"\"\"\n",
    "        # Format existing arguments for the prompt\n",
    "        existing_args_text = self._format_arguments_for_synthesis(self.current_arguments)\n",
    "        \n",
    "        # Format new comments\n",
    "        numbered_comments = [f\"[{idx}] {comment}\" for idx, comment in zip(new_indices, new_comments)]\n",
    "        new_comments_text = \"\\n\".join(numbered_comments)\n",
    "        \n",
    "        prompt = self.SYNTHESIS_TEMPLATE.format(\n",
    "            topic=topic,\n",
    "            existing_arguments=existing_args_text,\n",
    "            new_comments=new_comments_text\n",
    "        )\n",
    "        \n",
    "        response = await self.llm_provider.generate([Message(Role.USER, prompt)])\n",
    "        return await self._parse_arguments(response.content, likert_answers or [])\n",
    "\n",
    "    def _format_arguments_for_synthesis(self, arguments: List[Argument]) -> str:\n",
    "        \"\"\"Format existing arguments for the synthesis prompt.\"\"\"\n",
    "        formatted = []\n",
    "        for arg in arguments:\n",
    "            formatted.append(f\"MAIN: {arg.main_point} [source_indices: {','.join(map(str, arg.source_indices))}]\")\n",
    "            for sub in arg.subpoints:\n",
    "                formatted.append(f\"SUB: {sub} [source_indices: {','.join(map(str, arg.source_indices))}]\")\n",
    "        return \"\\n\".join(formatted)\n",
    "\n",
    "    async def _parse_arguments(self, response: str, likert_answers: List[int]) -> List[Argument]:\n",
    "        \"\"\"Parse LLM response into Argument objects with score distributions.\"\"\"\n",
    "        arguments = []\n",
    "        current_main = None\n",
    "        current_subpoints = []\n",
    "        current_indices = set()\n",
    "\n",
    "        pattern = r'<ARGUMENTS>(.*?)</ARGUMENTS>'\n",
    "        match = re.search(pattern, response, re.DOTALL)\n",
    "\n",
    "        # return early if the correct regex pattern is not found in the answer\n",
    "        if not match:\n",
    "            return arguments\n",
    "\n",
    "        lines = match.group(1).strip().split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            indices_match = re.search(r'\\[source_indices: ([\\d,\\s]+)\\]', line)\n",
    "            indices = [int(idx.strip()) for idx in indices_match.group(1).split(',')] if indices_match else []\n",
    "            \n",
    "            content = re.sub(r'\\[source_indices: [\\d,\\s]+\\]', '', line).strip()\n",
    "\n",
    "            if line.startswith('MAIN:'):\n",
    "                if current_main is not None:\n",
    "                    score_distribution = self._calculate_score_distribution(\n",
    "                        self._get_relevant_scores(list(current_indices), likert_answers)\n",
    "                    )\n",
    "                    arguments.append(Argument(\n",
    "                        main_point=current_main,\n",
    "                        subpoints=current_subpoints,\n",
    "                        score_distribution=score_distribution,\n",
    "                        source_indices=list(current_indices)\n",
    "                    ))\n",
    "\n",
    "                current_main = content.replace('MAIN:', '').strip()\n",
    "                current_subpoints = []\n",
    "                current_indices = set(indices)\n",
    "            \n",
    "            elif line.startswith('SUB:'):\n",
    "                subpoint = content.replace('SUB:', '').strip()\n",
    "                current_subpoints.append(subpoint)\n",
    "                current_indices.update(indices)\n",
    "\n",
    "        # Add the last argument\n",
    "        if current_main is not None:\n",
    "            score_distribution = self._calculate_score_distribution(\n",
    "                self._get_relevant_scores(list(current_indices), likert_answers)\n",
    "            )\n",
    "            arguments.append(Argument(\n",
    "                main_point=current_main,\n",
    "                subpoints=current_subpoints,\n",
    "                score_distribution=score_distribution,\n",
    "                source_indices=list(current_indices)\n",
    "            ))\n",
    "\n",
    "        return arguments\n",
    "\n",
    "    def _get_relevant_scores(self, indices: List[int], all_likert_scores: List[int]) -> List[int]:\n",
    "        \"\"\"Get Likert scores for given indices.\"\"\"\n",
    "        return [all_likert_scores[idx] for idx in indices if idx < len(all_likert_scores)]\n",
    "\n",
    "    def _calculate_score_distribution(self, scores: List[int]) -> List[int]:\n",
    "        \"\"\"Calculate distribution of Likert scores.\"\"\"\n",
    "        if not scores:\n",
    "            return [0] * 5\n",
    "\n",
    "        distribution = [0] * 5\n",
    "        for score in scores:\n",
    "            if 1 <= score <= 5:\n",
    "                distribution[int(score-1)] += 1\n",
    "\n",
    "        return distribution\n",
    "    \n",
    "    async def format_arguments(self, arguments: List[Argument]) -> str:\n",
    "        formatted_output = [\"-\" * 50]\n",
    "        for i, arg in enumerate(arguments, 1):\n",
    "            formatted_output.append(f\"Argument {i}: {arg.main_point}\\n\")\n",
    "            \n",
    "            if arg.subpoints:\n",
    "                formatted_output.append(\"Supporting points:\")\n",
    "                for j, subpoint in enumerate(arg.subpoints, 1):\n",
    "                    formatted_output.append(f\"  {j}. {subpoint}\")\n",
    "            \n",
    "            if arg.score_distribution:\n",
    "                formatted_output.append(\"\\nLikert Score Distribution (calculated from indices given by the LLM):\")\n",
    "                for score_nominality, prevalence in enumerate(arg.score_distribution, 1):\n",
    "                    formatted_output.append(f\"  Score {score_nominality}: {prevalence:} answers\")\n",
    "\n",
    "            formatted_output.append(f\"\\nSource indices: {arg.source_indices}\\n\")\n",
    "            \n",
    "            formatted_output.append(\"-\" * 50)\n",
    "        \n",
    "        return \"\\n\".join(formatted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing of 200 comments in batches of 20\n",
      "\n",
      "Processing complete!\n",
      "Results saved to: /Users/max/projects/learning/aalto/openvaa/argument-condensation/condensation/results/synthesis/arguments_20250101_153326.txt\n",
      "Processing history saved to: /Users/max/projects/learning/aalto/openvaa/argument-condensation/condensation/results/synthesis/processing_history_20250101_153326.json\n",
      "Detailed analysis saved to: /Users/max/projects/learning/aalto/openvaa/argument-condensation/condensation/results/synthesis/analysis_20250101_153326.txt\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    # Params\n",
    "    n_comments = 200  # Number of comments to process overall\n",
    "    batch_size = 20  # Number of comments per batch\n",
    "    topic = \"Kun kunnan menoja ja tuloja tasapainotetaan, se on tehtävä mieluummin menoja karsimalla kuin veroja kiristämällä.\"\n",
    "\n",
    "    # Config\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    model = \"gpt-4o-2024-11-20\"\n",
    "    openai_provider = OpenAIProvider(api_key, model)\n",
    "    processor = ArgumentProcessor(openai_provider, batch_size)\n",
    "\n",
    "    # Setup paths\n",
    "    data_source_path = os.path.join(parent_dir, 'data', 'sources', 'kuntavaalit2021.csv')\n",
    "    output_base_path = os.path.join(parent_dir, 'condensation', 'results', 'synthesis')\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs(output_base_path, exist_ok=True)\n",
    "    \n",
    "    # Define output paths\n",
    "    results_path = os.path.join(output_base_path, f'arguments_{timestamp}.txt')\n",
    "    history_path = os.path.join(output_base_path, f'processing_history_{timestamp}.json')\n",
    "    analysis_path = os.path.join(output_base_path, f'analysis_{timestamp}.txt')\n",
    "\n",
    "    # Read and prepare data\n",
    "    df = pd.read_csv(data_source_path)\n",
    "\n",
    "    # Choose a subset of comments to process\n",
    "    question_index = 10\n",
    "    explanation_column_name = f'q{question_index}.explanation_fi'\n",
    "    likert_column_name = f'q{question_index}.answer'\n",
    "\n",
    "    # Get comments and their original indices\n",
    "    comment_mask = df[explanation_column_name].notna()  # Only process comments with content\n",
    "    comment_indices = df[comment_mask][explanation_column_name].index[:n_comments].tolist()\n",
    "    comments = df.loc[comment_indices, explanation_column_name].tolist()\n",
    "    likert_answers = df.loc[comment_indices, likert_column_name].tolist()\n",
    "\n",
    "    print(f\"Starting processing of {len(comments)} comments in batches of {batch_size}\")\n",
    "    \n",
    "    # Process all comments in batches\n",
    "    start_time = datetime.datetime.now()\n",
    "    final_arguments = await processor.process_all_comments(\n",
    "        comments=comments,\n",
    "        comment_indices=comment_indices,\n",
    "        topic=topic,\n",
    "        likert_answers=likert_answers\n",
    "    )\n",
    "    processor.current_arguments = final_arguments\n",
    "    total_processing_time = (datetime.datetime.now()- start_time).total_seconds()\n",
    "\n",
    "    # Export processing history\n",
    "    processor.export_processing_history(history_path)\n",
    "    \n",
    "    # Get batch statistics\n",
    "    stats = processor.get_batch_statistics()\n",
    "    \n",
    "    # Format and save main results\n",
    "    formatted_args = await processor.format_arguments(final_arguments)\n",
    "    with open(results_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"Processing Summary:\\n\")\n",
    "        f.write(f\"- Total comments processed: {len(comments)}\\n\")\n",
    "        f.write(f\"- Number of batches: {len(stats['batch_numbers'])}\\n\")\n",
    "        f.write(f\"- Total processing time: {total_processing_time:.2f} seconds\\n\")\n",
    "        f.write(f\"- Final number of arguments: {len(final_arguments)}\\n\\n\")\n",
    "        f.write(formatted_args)\n",
    "\n",
    "    # for each comment index, check how many arguments it was used in when creating the arguments\n",
    "    index_usage = {}\n",
    "    for i, arg in enumerate(final_arguments):\n",
    "        for idx in arg.source_indices:\n",
    "            if idx not in index_usage:\n",
    "                index_usage[idx] = 0\n",
    "            index_usage[idx] += 1\n",
    "\n",
    "    # Save detailed analysis\n",
    "    with open(analysis_path, 'w', encoding='utf-8') as f:\n",
    "        # Write overall statistics\n",
    "        f.write(\"=== Processing Statistics ===\\n\")\n",
    "        f.write(f\"Total processing time: {total_processing_time:.2f} seconds\\n\")\n",
    "        f.write(f\"Average batch processing time: {sum(stats['processing_times'])/len(stats['processing_times']):.2f} seconds\\n\")\n",
    "        f.write(f\"Total comments processed: {sum(stats['num_comments'])}\\n\\n\")\n",
    "        \n",
    "        # Write argument evolution analysis\n",
    "        f.write(\"\\n=== Argument Evolution ===\\n\")\n",
    "        for i, arg in enumerate(final_arguments):\n",
    "            f.write(f\"\\nArgument {i+1} Evolution:\\n\")\n",
    "            evolution = processor.get_argument_evolution(i)\n",
    "            for j, version in enumerate(evolution):\n",
    "                f.write(f\"Batch {j}: {version}\\n\")\n",
    "\n",
    "        # Write index usage analysis\n",
    "        f.write(\"\\n=== Index Usage Analysis ===\\n\")\n",
    "        for idx, count in index_usage.items():\n",
    "            f.write(f\"Comment index {idx} used in {count} arguments\\n\")\n",
    "        \n",
    "        # Write sample validation of final arguments\n",
    "        f.write(\"\\n=== Sample Validation of Final Arguments ===\\n\")\n",
    "        for i, arg in enumerate(final_arguments):\n",
    "            f.write(f\"\\nArgument {i+1}: {arg.main_point}\\n\")\n",
    "            f.write(\"Sample supporting comments:\\n\")\n",
    "            for idx in arg.source_indices[:5]:  # Show first 5 supporting comments\n",
    "                comment = df.loc[idx, explanation_column_name]\n",
    "                f.write(f\"[{idx}] {comment}\\n\")\n",
    "\n",
    "        # Write batch-by-batch statistics\n",
    "        f.write(\"=== Batch-by-Batch Statistics ===\\n\")\n",
    "        for i in range(len(stats['batch_numbers'])):\n",
    "            f.write(f\"\\nBatch {stats['batch_numbers'][i]}:\\n\")\n",
    "            f.write(f\"- Timestamp: {stats['timestamps'][i]}\\n\")\n",
    "            f.write(f\"- Comments processed: {stats['num_comments'][i]}\\n\")\n",
    "            f.write(f\"- Arguments after batch: {stats['num_arguments'][i]}\\n\")\n",
    "            f.write(f\"- Processing time: {stats['processing_times'][i]:.2f} seconds\\n\")\n",
    "            f.write(f\"- Total subpoints: {stats['total_subpoints'][i]}\\n\")\n",
    "        \n",
    "\n",
    "    print(f\"\\nProcessing complete!\")\n",
    "    print(f\"Results saved to: {results_path}\")\n",
    "    print(f\"Processing history saved to: {history_path}\")\n",
    "    print(f\"Detailed analysis saved to: {analysis_path}\")\n",
    "\n",
    "# run\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
